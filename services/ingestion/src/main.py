"""
main.py
üèÅ Entry Point c·ªßa Service Ingestion
H·ªó tr·ª£ 3 modes: BACKFILL, HOURLY, COMPACTION
"""
import logging
import sys
from datetime import datetime, timedelta
from typing import List

from config import Config
from s3_writer import S3Writer
from api_clients.weather import WeatherAPIClient
from api_clients.electricity import ElectricityAPIClient
from compactor import DataCompactor

# Setup logging
logging.basicConfig(
    level=Config.LOG_LEVEL,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

def generate_date_list(start_date: str, end_date: str) -> List[str]:
    """
    T·∫°o list c√°c ng√†y t·ª´ start_date ƒë·∫øn end_date
    
    Args:
        start_date: Ng√†y b·∫Øt ƒë·∫ßu (format: YYYY-MM-DD)
        end_date: Ng√†y k·∫øt th√∫c (format: YYYY-MM-DD)
    
    Returns:
        List[str]: List c√°c ng√†y
    """
    start = datetime.strptime(start_date, "%Y-%m-%d")
    end = datetime.strptime(end_date, "%Y-%m-%d")
    
    date_list = []
    current = start
    
    while current <= end:
        date_list.append(current.strftime("%Y-%m-%d"))
        current += timedelta(days=1)
    
    return date_list

def ingest_weather_data_backfill(
    weather_client: WeatherAPIClient,
    s3_writer: S3Writer,
    date_list: List[str]
) -> dict:
    """
    Ingest weather data cho BACKFILL mode (to√†n b·ªô ng√†y, l∆∞u 1 file)
    
    Args:
        weather_client: Weather API client
        s3_writer: S3 writer instance
        date_list: List c√°c ng√†y c·∫ßn l·∫•y
    
    Returns:
        dict: K·∫øt qu·∫£ th·ªëng k√™ {success: int, failed: int}
    """
    logger.info(f"‚òÄÔ∏è Starting weather data ingestion (BACKFILL) for {len(date_list)} days")
    
    stats = {"success": 0, "failed": 0, "skipped": 0}
    
    for idx, date in enumerate(date_list, 1):
        try:
            logger.info(f"üìÖ [{idx}/{len(date_list)}] Processing {date}")
            
            # Check if file already exists
            s3_key = s3_writer._generate_partition_path("weather", date, hour=None)
            if s3_writer.check_file_exists(s3_key):
                logger.info(f"‚è≠Ô∏è File already exists, skipping...")
                stats["skipped"] += 1
                continue
            
            # Fetch data from API (full day data)
            data = weather_client.fetch_data(date)
            
            # Write to S3 (hour=None -> data.json)
            s3_uri = s3_writer.write_weather_data(data, date, hour=None)
            
            logger.info(f"‚úÖ [{idx}/{len(date_list)}] {date} -> {s3_uri}")
            stats["success"] += 1
            
        except Exception as e:
            logger.error(f"‚ùå [{idx}/{len(date_list)}] Failed to process {date}: {str(e)}")
            stats["failed"] += 1
    
    logger.info(f"‚òÄÔ∏è Weather ingestion (BACKFILL) completed: {stats}")
    return stats

def ingest_weather_data_hourly(
    weather_client: WeatherAPIClient,
    s3_writer: S3Writer,
    target_date: str,
    target_hour: str
) -> dict:
    """
    Ingest weather data cho HOURLY mode (1 gi·ªù, l∆∞u file ri√™ng)
    
    Args:
        weather_client: Weather API client
        s3_writer: S3 writer instance
        target_date: Ng√†y c·∫ßn l·∫•y (format: YYYY-MM-DD)
        target_hour: Gi·ªù c·∫ßn l·∫•y (format: HH)
    
    Returns:
        dict: K·∫øt qu·∫£ th·ªëng k√™
    """
    logger.info(f"‚òÄÔ∏è Starting weather data ingestion (HOURLY) for {target_date} {target_hour}:00")
    
    stats = {"success": 0, "failed": 0, "skipped": 0}
    
    try:
        # Check if file already exists
        s3_key = s3_writer._generate_partition_path("weather", target_date, hour=target_hour)
        if s3_writer.check_file_exists(s3_key):
            logger.info(f"‚è≠Ô∏è File already exists, skipping...")
            stats["skipped"] = 1
            return stats
        
        # Fetch data from API (full day, will extract 1 hour)
        data = weather_client.fetch_data(target_date)
        
        # Extract only the target hour
        if 'days' in data and len(data['days']) > 0:
            day_data = data['days'][0]
            if 'hours' in day_data:
                # Find the specific hour
                target_hour_data = None
                for hour_data in day_data['hours']:
                    hour_str = hour_data['datetime'].split(':')[0]  # "13:00:00" -> "13"
                    if hour_str == target_hour:
                        target_hour_data = hour_data
                        break
                
                if target_hour_data:
                    # Create single-hour structure
                    hourly_data = {
                        k: v for k, v in data.items() 
                        if k != 'days'
                    }
                    hourly_data['days'] = [{
                        'datetime': target_date,
                        'hours': [target_hour_data]
                    }]
                    
                    # Write to S3 with hour specification
                    s3_uri = s3_writer.write_weather_data(hourly_data, target_date, hour=target_hour)
                    
                    logger.info(f"‚úÖ {target_date} {target_hour}:00 -> {s3_uri}")
                    stats["success"] = 1
                else:
                    logger.error(f"‚ùå Hour {target_hour} not found in API response")
                    stats["failed"] = 1
            else:
                logger.error(f"‚ùå No hourly data in API response")
                stats["failed"] = 1
        else:
            logger.error(f"‚ùå Invalid API response structure")
            stats["failed"] = 1
            
    except Exception as e:
        logger.error(f"‚ùå Failed to process {target_date} {target_hour}:00: {str(e)}")
        stats["failed"] = 1
    
    logger.info(f"‚òÄÔ∏è Weather ingestion (HOURLY) completed: {stats}")
    return stats

def ingest_electricity_data_backfill(
    electricity_client: ElectricityAPIClient,
    s3_writer: S3Writer,
    date_list: List[str],
    signal_list: List[str]
) -> dict:
    """
    Ingest electricity data cho BACKFILL mode
    
    Args:
        electricity_client: Electricity API client
        s3_writer: S3 writer instance
        date_list: List c√°c ng√†y c·∫ßn l·∫•y
        signal_list: List c√°c signals c·∫ßn l·∫•y
    
    Returns:
        dict: K·∫øt qu·∫£ th·ªëng k√™
    """
    logger.info(f"‚ö° Starting electricity data ingestion (BACKFILL) for {len(date_list)} days x {len(signal_list)} signals")
    
    stats = {"success": 0, "failed": 0, "skipped": 0}
    
    for date_idx, date in enumerate(date_list, 1):
        logger.info(f"üìÖ [{date_idx}/{len(date_list)}] Processing {date}")
        
        for signal_idx, signal in enumerate(signal_list, 1):
            try:
                logger.info(f"  ‚ö° [{signal_idx}/{len(signal_list)}] Fetching {signal}")
                
                # Check if file already exists
                s3_key = s3_writer._generate_partition_path("electricity", date, signal, hour=None)
                if s3_writer.check_file_exists(s3_key):
                    logger.info(f"  ‚è≠Ô∏è File already exists, skipping...")
                    stats["skipped"] += 1
                    continue
                
                # Fetch data from API
                data = electricity_client.fetch_data(date, signal)
                
                # Write to S3
                s3_uri = s3_writer.write_electricity_data(data, signal, date, hour=None)
                
                logger.info(f"  ‚úÖ {signal} -> {s3_uri}")
                stats["success"] += 1
                
            except Exception as e:
                logger.error(f"  ‚ùå Failed to process {signal} for {date}: {str(e)}")
                stats["failed"] += 1
    
    logger.info(f"‚ö° Electricity ingestion (BACKFILL) completed: {stats}")
    return stats

def ingest_electricity_data_hourly(
    electricity_client: ElectricityAPIClient,
    s3_writer: S3Writer,
    target_date: str,
    target_hour: str,
    signal_list: List[str]
) -> dict:
    """
    Ingest electricity data cho HOURLY mode
    
    Args:
        electricity_client: Electricity API client
        s3_writer: S3 writer instance
        target_date: Ng√†y c·∫ßn l·∫•y
        target_hour: Gi·ªù c·∫ßn l·∫•y
        signal_list: List c√°c signals
    
    Returns:
        dict: K·∫øt qu·∫£ th·ªëng k√™
    """
    logger.info(f"‚ö° Starting electricity data ingestion (HOURLY) for {target_date} {target_hour}:00")
    
    stats = {"success": 0, "failed": 0, "skipped": 0}
    
    for signal_idx, signal in enumerate(signal_list, 1):
        try:
            logger.info(f"  ‚ö° [{signal_idx}/{len(signal_list)}] Fetching {signal}")
            
            # Check if file already exists
            s3_key = s3_writer._generate_partition_path("electricity", target_date, signal, hour=target_hour)
            if s3_writer.check_file_exists(s3_key):
                logger.info(f"  ‚è≠Ô∏è File already exists, skipping...")
                stats["skipped"] += 1
                continue
            
            # Fetch data from API (full day)
            data = electricity_client.fetch_data(target_date, signal)
            
            # Extract only target hour
            if 'history' in data:
                target_hour_data = None
                for record in data['history']:
                    datetime_str = record['datetime']  # "2024-01-11T13:00:00Z"
                    hour_str = datetime_str.split('T')[1].split(':')[0]
                    if hour_str == target_hour:
                        target_hour_data = record
                        break
                
                if target_hour_data:
                    # Create single-hour structure
                    hourly_data = {
                        k: v for k, v in data.items() 
                        if k not in ['history', '_metadata']
                    }
                    hourly_data['history'] = [target_hour_data]
                    hourly_data['_metadata'] = {
                        "signal": signal,
                        "query_date": target_date,
                        "hour": target_hour,
                        "zone": Config.ELECTRICITY_ZONE
                    }
                    
                    # Write to S3
                    s3_uri = s3_writer.write_electricity_data(hourly_data, signal, target_date, hour=target_hour)
                    
                    logger.info(f"  ‚úÖ {signal} -> {s3_uri}")
                    stats["success"] += 1
                else:
                    logger.error(f"  ‚ùå Hour {target_hour} not found for {signal}")
                    stats["failed"] += 1
            else:
                logger.error(f"  ‚ùå No history data for {signal}")
                stats["failed"] += 1
                
        except Exception as e:
            logger.error(f"  ‚ùå Failed to process {signal}: {str(e)}")
            stats["failed"] += 1
    
    logger.info(f"‚ö° Electricity ingestion (HOURLY) completed: {stats}")
    return stats

def run_backfill_mode():
    """
    Ch·∫°y BACKFILL mode: L·∫•y to√†n b·ªô d·ªØ li·ªáu l·ªãch s·ª≠
    """
    logger.info("=" * 60)
    logger.info("MODE: BACKFILL")
    logger.info("=" * 60)
    
    # Get date range
    start_date, end_date = Config.get_date_range()
    date_list = generate_date_list(start_date, end_date)
    logger.info(f"üìÖ Date range: {start_date} to {end_date} ({len(date_list)} days)")
    
    # Initialize clients
    logger.info("üîß Initializing API clients...")
    
    weather_client = WeatherAPIClient(
        api_key=Config.VISUAL_CROSSING_API_KEY,
        api_host=Config.WEATHER_API_HOST,
        location=Config.WEATHER_LOCATION,
        elements=Config.WEATHER_ELEMENTS,
        max_retries=Config.MAX_RETRIES
    )
    
    electricity_client = ElectricityAPIClient(
        api_key=Config.ELECTRICITY_MAPS_API_KEY,
        api_host=Config.ELECTRICITY_API_HOST,
        zone=Config.ELECTRICITY_ZONE,
        granularity=Config.ELECTRICITY_GRANULARITY,
        endpoint_mapping=Config.ENDPOINT_MAPPING,
        max_retries=Config.MAX_RETRIES
    )
    
    s3_writer = S3Writer(
        bucket_name=Config.S3_BUCKET,
        bronze_prefix=Config.S3_BRONZE_PREFIX
    )
    
    # Ingest data
    logger.info("=" * 60)
    logger.info("STEP 1: WEATHER DATA INGESTION")
    logger.info("=" * 60)
    weather_stats = ingest_weather_data_backfill(weather_client, s3_writer, date_list)
    
    logger.info("=" * 60)
    logger.info("STEP 2: ELECTRICITY DATA INGESTION")
    logger.info("=" * 60)
    electricity_stats = ingest_electricity_data_backfill(
        electricity_client, 
        s3_writer, 
        date_list,
        Config.ELECTRICITY_SIGNALS
    )
    
    return weather_stats, electricity_stats

def run_hourly_mode():
    """
    Ch·∫°y HOURLY mode: L·∫•y d·ªØ li·ªáu c·ªßa gi·ªù tr∆∞·ªõc
    """
    logger.info("=" * 60)
    logger.info("MODE: HOURLY")
    logger.info("=" * 60)
    
    # Get target datetime
    target_date, target_hour = Config.get_target_datetime()
    logger.info(f"üéØ Target: {target_date} {target_hour}:00")
    
    # Initialize clients
    logger.info("üîß Initializing API clients...")
    
    weather_client = WeatherAPIClient(
        api_key=Config.VISUAL_CROSSING_API_KEY,
        api_host=Config.WEATHER_API_HOST,
        location=Config.WEATHER_LOCATION,
        elements=Config.WEATHER_ELEMENTS,
        max_retries=Config.MAX_RETRIES
    )
    
    electricity_client = ElectricityAPIClient(
        api_key=Config.ELECTRICITY_MAPS_API_KEY,
        api_host=Config.ELECTRICITY_API_HOST,
        zone=Config.ELECTRICITY_ZONE,
        granularity=Config.ELECTRICITY_GRANULARITY,
        endpoint_mapping=Config.ENDPOINT_MAPPING,
        max_retries=Config.MAX_RETRIES
    )
    
    s3_writer = S3Writer(
        bucket_name=Config.S3_BUCKET,
        bronze_prefix=Config.S3_BRONZE_PREFIX
    )
    
    # Ingest data
    logger.info("=" * 60)
    logger.info("STEP 1: WEATHER DATA INGESTION")
    logger.info("=" * 60)
    weather_stats = ingest_weather_data_hourly(
        weather_client, 
        s3_writer, 
        target_date, 
        target_hour
    )
    
    logger.info("=" * 60)
    logger.info("STEP 2: ELECTRICITY DATA INGESTION")
    logger.info("=" * 60)
    electricity_stats = ingest_electricity_data_hourly(
        electricity_client,
        s3_writer,
        target_date,
        target_hour,
        Config.ELECTRICITY_SIGNALS
    )
    
    return weather_stats, electricity_stats

def run_compaction_mode():
    """
    Ch·∫°y COMPACTION mode: G·ªôp hourly files c·ªßa ng√†y h√¥m qua
    """
    logger.info("=" * 60)
    logger.info("MODE: COMPACTION")
    logger.info("=" * 60)
    
    # Get yesterday's date
    start_date, end_date = Config.get_date_range()
    logger.info(f"üéØ Compacting data for: {start_date}")
    
    # Initialize S3 writer and compactor
    s3_writer = S3Writer(
        bucket_name=Config.S3_BUCKET,
        bronze_prefix=Config.S3_BRONZE_PREFIX
    )
    
    compactor = DataCompactor(s3_writer)
    
    # Run compaction
    results = compactor.compact_all(start_date)
    
    # Check results
    success = True
    if results['weather']['status'] != 'success':
        logger.error(f"‚ùå Weather compaction failed")
        success = False
    
    for signal, result in results['electricity'].items():
        if result['status'] != 'success':
            logger.error(f"‚ùå {signal} compaction failed")
            success = False
    
    if success:
        logger.info("‚úÖ All compactions completed successfully")
    
    return results

def main():
    """
    Main orchestrator function
    """
    try:
        # Validate config
        logger.info("üîç Validating configuration...")
        Config.validate()
        
        mode = Config.get_mode()
        logger.info(f"‚úÖ Config OK - Mode: {mode}")
        
        # Route to appropriate mode handler
        if mode == "BACKFILL":
            weather_stats, electricity_stats = run_backfill_mode()
            
            # Final report
            logger.info("=" * 60)
            logger.info("üéâ BACKFILL COMPLETED")
            logger.info("=" * 60)
            logger.info(f"Weather: {weather_stats}")
            logger.info(f"Electricity: {electricity_stats}")
            
            total_failed = weather_stats["failed"] + electricity_stats["failed"]
            if total_failed > 0:
                logger.warning(f"‚ö†Ô∏è {total_failed} tasks failed")
                sys.exit(1)
            else:
                logger.info("‚úÖ All tasks completed successfully")
                sys.exit(0)
        
        elif mode == "HOURLY":
            weather_stats, electricity_stats = run_hourly_mode()
            
            # Final report
            logger.info("=" * 60)
            logger.info("üéâ HOURLY INGESTION COMPLETED")
            logger.info("=" * 60)
            logger.info(f"Weather: {weather_stats}")
            logger.info(f"Electricity: {electricity_stats}")
            
            total_failed = weather_stats["failed"] + electricity_stats["failed"]
            if total_failed > 0:
                logger.warning(f"‚ö†Ô∏è {total_failed} tasks failed")
                sys.exit(1)
            else:
                logger.info("‚úÖ All tasks completed successfully")
                sys.exit(0)
        
        elif mode == "COMPACTION":
            results = run_compaction_mode()
            
            # Final report
            logger.info("=" * 60)
            logger.info("üéâ COMPACTION COMPLETED")
            logger.info("=" * 60)
            logger.info(f"Results: {results}")
            
            # Check for failures
            has_error = False
            if results['weather']['status'] != 'success':
                has_error = True
            for signal, result in results['electricity'].items():
                if result['status'] != 'success':
                    has_error = True
            
            if has_error:
                logger.warning("‚ö†Ô∏è Some compactions failed")
                sys.exit(1)
            else:
                logger.info("‚úÖ All compactions completed successfully")
                sys.exit(0)
        
    except Exception as e:
        logger.error(f"üí• Fatal error: {str(e)}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main()